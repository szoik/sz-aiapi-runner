{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment Sampling: Error Analysis\n",
        "\n",
        "Compare actual vs AI estimated values with proper dimension handling.\n",
        "\n",
        "**Key Insight:**\n",
        "- Volume alone can hide errors (1x20x1 vs 2x5x2 = same volume 20, different shape)\n",
        "- Solution: Sort dimensions (L >= W >= H) and compare individually\n",
        "\n",
        "**Metrics:**\n",
        "- Weight error\n",
        "- Volume error\n",
        "- L/W/H individual errors (sorted dimensions)\n",
        "- Average dimension error\n",
        "\n",
        "**Sample Sets:**\n",
        "- Error 0.5 ~ 1.0 (50-100%)\n",
        "- Error > 1.0 (over 100%)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-cloud-bigquery pandas matplotlib seaborn"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "PROJECT_ID = \"sazoshop\"\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "def run_query(sql):\n",
        "    return client.query(sql).to_dataframe()\n",
        "\n",
        "print(\"✅ Ready\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load All Data with Error Metrics"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Main query: Calculate all error metrics with sorted dimensions\n",
        "QUERY = \"\"\"\n",
        "WITH single_item_orders AS (\n",
        "  SELECT order_item_order_id\n",
        "  FROM `sazoshop.firestore_snapshot.v2_order_items`\n",
        "  GROUP BY order_item_order_id\n",
        "  HAVING COUNT(DISTINCT order_item_id) = 1\n",
        "),\n",
        "\n",
        "base_data AS (\n",
        "  SELECT\n",
        "    oi.order_item_order_id,\n",
        "    oi.order_item_title_origin AS title,\n",
        "    oi.order_item_product_version_info_category AS category,\n",
        "    ARRAY_TO_STRING(oi.order_item_product_version_thumbnail_urls, '|') AS thumbnail_urls,\n",
        "    \n",
        "    -- Actual measurements (from KSE)\n",
        "    kse.actual_weight,\n",
        "    CAST(SPLIT(kse.dimensions, 'x')[OFFSET(0)] AS FLOAT64) AS actual_d1,\n",
        "    CAST(SPLIT(kse.dimensions, 'x')[OFFSET(1)] AS FLOAT64) AS actual_d2,\n",
        "    CAST(SPLIT(kse.dimensions, 'x')[OFFSET(2)] AS FLOAT64) AS actual_d3,\n",
        "    kse.dimensions AS actual_dimensions_raw,\n",
        "    \n",
        "    -- AI estimated measurements\n",
        "    SAFE_CAST(JSON_EXTRACT_SCALAR(oi.order_item_product_version_extra, '$.weight') AS FLOAT64) AS ai_weight,\n",
        "    SAFE_CAST(JSON_EXTRACT_SCALAR(oi.order_item_product_version_extra, '$.width') AS FLOAT64) AS ai_d1,\n",
        "    SAFE_CAST(JSON_EXTRACT_SCALAR(oi.order_item_product_version_extra, '$.depth') AS FLOAT64) AS ai_d2,\n",
        "    SAFE_CAST(JSON_EXTRACT_SCALAR(oi.order_item_product_version_extra, '$.height') AS FLOAT64) AS ai_d3,\n",
        "    \n",
        "    kse.shipping_date\n",
        "    \n",
        "  FROM `sazoshop.firestore_snapshot.v2_order_items` oi\n",
        "  INNER JOIN `sazoshop.firestore_collection.v2_kse_cost` kse\n",
        "    ON oi.order_item_order_id = kse.order_id\n",
        "  WHERE \n",
        "    oi.order_item_order_id IN (SELECT order_item_order_id FROM single_item_orders)\n",
        "    AND kse.dimensions IS NOT NULL\n",
        "    AND kse.actual_weight IS NOT NULL\n",
        "    AND kse.actual_weight > 0\n",
        "    AND kse.actual_weight < 100\n",
        "    AND REGEXP_CONTAINS(kse.dimensions, r'^[0-9.]+x[0-9.]+x[0-9.]+$')\n",
        "    AND oi.order_item_title_origin IS NOT NULL\n",
        "    AND JSON_EXTRACT_SCALAR(oi.order_item_product_version_extra, '$.weight') IS NOT NULL\n",
        "),\n",
        "\n",
        "sorted_dimensions AS (\n",
        "  SELECT\n",
        "    *,\n",
        "    -- Sort actual dimensions (descending: L >= W >= H)\n",
        "    GREATEST(actual_d1, actual_d2, actual_d3) AS actual_L,\n",
        "    (actual_d1 + actual_d2 + actual_d3) \n",
        "      - GREATEST(actual_d1, actual_d2, actual_d3) \n",
        "      - LEAST(actual_d1, actual_d2, actual_d3) AS actual_W,\n",
        "    LEAST(actual_d1, actual_d2, actual_d3) AS actual_H,\n",
        "    \n",
        "    -- Sort AI dimensions (descending: L >= W >= H)\n",
        "    GREATEST(IFNULL(ai_d1,0), IFNULL(ai_d2,0), IFNULL(ai_d3,0)) AS ai_L,\n",
        "    (IFNULL(ai_d1,0) + IFNULL(ai_d2,0) + IFNULL(ai_d3,0)) \n",
        "      - GREATEST(IFNULL(ai_d1,0), IFNULL(ai_d2,0), IFNULL(ai_d3,0)) \n",
        "      - LEAST(IFNULL(ai_d1,0), IFNULL(ai_d2,0), IFNULL(ai_d3,0)) AS ai_W,\n",
        "    LEAST(IFNULL(ai_d1,0), IFNULL(ai_d2,0), IFNULL(ai_d3,0)) AS ai_H,\n",
        "    \n",
        "    -- Calculate volumes\n",
        "    (actual_d1 * actual_d2 * actual_d3) / 1000000 AS actual_volume_m3,\n",
        "    (IFNULL(ai_d1,0) * IFNULL(ai_d2,0) * IFNULL(ai_d3,0)) / 1000000 AS ai_volume_m3\n",
        "    \n",
        "  FROM base_data\n",
        "),\n",
        "\n",
        "with_errors AS (\n",
        "  SELECT\n",
        "    *,\n",
        "    -- Weight error\n",
        "    CASE WHEN actual_weight > 0 \n",
        "      THEN ABS(ai_weight - actual_weight) / actual_weight \n",
        "      ELSE NULL END AS weight_error,\n",
        "    \n",
        "    -- Volume error\n",
        "    CASE WHEN actual_volume_m3 > 0 \n",
        "      THEN ABS(ai_volume_m3 - actual_volume_m3) / actual_volume_m3 \n",
        "      ELSE NULL END AS volume_error,\n",
        "    \n",
        "    -- Individual dimension errors (sorted)\n",
        "    CASE WHEN actual_L > 0 \n",
        "      THEN ABS(ai_L - actual_L) / actual_L \n",
        "      ELSE NULL END AS L_error,\n",
        "    CASE WHEN actual_W > 0 \n",
        "      THEN ABS(ai_W - actual_W) / actual_W \n",
        "      ELSE NULL END AS W_error,\n",
        "    CASE WHEN actual_H > 0 \n",
        "      THEN ABS(ai_H - actual_H) / actual_H \n",
        "      ELSE NULL END AS H_error,\n",
        "    \n",
        "    -- Combined dimension error (average of L/W/H errors)\n",
        "    (\n",
        "      CASE WHEN actual_L > 0 THEN ABS(ai_L - actual_L) / actual_L ELSE 0 END +\n",
        "      CASE WHEN actual_W > 0 THEN ABS(ai_W - actual_W) / actual_W ELSE 0 END +\n",
        "      CASE WHEN actual_H > 0 THEN ABS(ai_H - actual_H) / actual_H ELSE 0 END\n",
        "    ) / 3 AS avg_dim_error\n",
        "    \n",
        "  FROM sorted_dimensions\n",
        "  WHERE ai_d1 IS NOT NULL\n",
        ")\n",
        "\n",
        "SELECT\n",
        "  order_item_order_id,\n",
        "  title,\n",
        "  category,\n",
        "  thumbnail_urls,\n",
        "  \n",
        "  -- Actual (sorted)\n",
        "  actual_weight,\n",
        "  actual_L, actual_W, actual_H,\n",
        "  ROUND(actual_volume_m3, 6) AS actual_volume_m3,\n",
        "  \n",
        "  -- AI estimated (sorted)\n",
        "  ai_weight,\n",
        "  ai_L, ai_W, ai_H,\n",
        "  ROUND(ai_volume_m3, 6) AS ai_volume_m3,\n",
        "  \n",
        "  -- Errors\n",
        "  ROUND(weight_error, 3) AS weight_error,\n",
        "  ROUND(volume_error, 3) AS volume_error,\n",
        "  ROUND(L_error, 3) AS L_error,\n",
        "  ROUND(W_error, 3) AS W_error,\n",
        "  ROUND(H_error, 3) AS H_error,\n",
        "  ROUND(avg_dim_error, 3) AS avg_dim_error,\n",
        "  \n",
        "  shipping_date\n",
        "\n",
        "FROM with_errors\n",
        "ORDER BY shipping_date DESC\n",
        "\"\"\"\n",
        "\n",
        "df = run_query(QUERY)\n",
        "print(f\"✅ Loaded {len(df):,} records with error metrics\")\n",
        "df.head()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Error Distribution Overview"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary statistics\n",
        "print(\"=== Error Statistics ===\")\n",
        "error_cols = ['weight_error', 'volume_error', 'L_error', 'W_error', 'H_error', 'avg_dim_error']\n",
        "df[error_cols].describe().round(3)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count by error range\n",
        "def categorize_error(x):\n",
        "    if pd.isna(x):\n",
        "        return 'N/A'\n",
        "    elif x < 0.5:\n",
        "        return '< 50%'\n",
        "    elif x < 1.0:\n",
        "        return '50-100%'\n",
        "    else:\n",
        "        return '> 100%'\n",
        "\n",
        "print(\"=== Error Category Counts ===\")\n",
        "for col in ['weight_error', 'volume_error', 'avg_dim_error']:\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(df[col].apply(categorize_error).value_counts())"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize error distributions\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "for ax, col, title in zip(axes, \n",
        "    ['weight_error', 'volume_error', 'avg_dim_error'],\n",
        "    ['Weight Error', 'Volume Error', 'Avg Dimension Error']):\n",
        "    \n",
        "    data = df[col].dropna()\n",
        "    data_capped = data.clip(upper=3)  # Cap at 300% for visualization\n",
        "    \n",
        "    ax.hist(data_capped, bins=30, edgecolor='black', alpha=0.7)\n",
        "    ax.axvline(x=0.5, color='orange', linestyle='--', label='50%')\n",
        "    ax.axvline(x=1.0, color='red', linestyle='--', label='100%')\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel('Error Rate')\n",
        "    ax.set_ylabel('Count')\n",
        "    ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Sample Sets by Error Type"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sample sets\n",
        "sample_sets = {\n",
        "    # By Weight Error\n",
        "    'weight_50_100': df[(df['weight_error'] >= 0.5) & (df['weight_error'] < 1.0)],\n",
        "    'weight_over_100': df[df['weight_error'] >= 1.0],\n",
        "    \n",
        "    # By Volume Error\n",
        "    'volume_50_100': df[(df['volume_error'] >= 0.5) & (df['volume_error'] < 1.0)],\n",
        "    'volume_over_100': df[df['volume_error'] >= 1.0],\n",
        "    \n",
        "    # By Dimension Error (L/W/H average)\n",
        "    'dim_50_100': df[(df['avg_dim_error'] >= 0.5) & (df['avg_dim_error'] < 1.0)],\n",
        "    'dim_over_100': df[df['avg_dim_error'] >= 1.0],\n",
        "}\n",
        "\n",
        "print(\"=== Sample Set Sizes ===\")\n",
        "for name, sample_df in sample_sets.items():\n",
        "    print(f\"{name}: {len(sample_df):,} items\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Explore High-Error Samples"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper to display samples with images\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_samples(df_subset, title, n=10):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{title} (showing {min(n, len(df_subset))} of {len(df_subset)})\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    html = \"<table style='font-size:12px;'>\"\n",
        "    html += \"<tr><th>Image</th><th>Title</th><th>Actual</th><th>AI Est</th><th>Errors</th></tr>\"\n",
        "    \n",
        "    for _, row in df_subset.head(n).iterrows():\n",
        "        img_url = row['thumbnail_urls'].split('|')[0] if row['thumbnail_urls'] else ''\n",
        "        img_html = f'<img src=\"{img_url}\" style=\"max-width:80px;\">' if img_url else 'N/A'\n",
        "        \n",
        "        actual = f\"W:{row['actual_weight']:.1f}kg<br>L:{row['actual_L']:.0f} W:{row['actual_W']:.0f} H:{row['actual_H']:.0f}\"\n",
        "        ai_est = f\"W:{row['ai_weight']:.1f}kg<br>L:{row['ai_L']:.0f} W:{row['ai_W']:.0f} H:{row['ai_H']:.0f}\"\n",
        "        errors = f\"Wt:{row['weight_error']*100:.0f}%<br>Vol:{row['volume_error']*100:.0f}%<br>Dim:{row['avg_dim_error']*100:.0f}%\"\n",
        "        \n",
        "        html += f\"<tr>\"\n",
        "        html += f\"<td>{img_html}</td>\"\n",
        "        html += f\"<td>{row['title'][:40]}...</td>\"\n",
        "        html += f\"<td>{actual}</td>\"\n",
        "        html += f\"<td>{ai_est}</td>\"\n",
        "        html += f\"<td>{errors}</td>\"\n",
        "        html += f\"</tr>\"\n",
        "    \n",
        "    html += \"</table>\"\n",
        "    display(HTML(html))"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show samples: Weight error 50-100%\n",
        "show_samples(sample_sets['weight_50_100'].sort_values('weight_error', ascending=False), \n",
        "             'Weight Error 50-100%')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show samples: Weight error > 100%\n",
        "show_samples(sample_sets['weight_over_100'].sort_values('weight_error', ascending=False), \n",
        "             'Weight Error > 100%')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show samples: Volume error > 100%\n",
        "show_samples(sample_sets['volume_over_100'].sort_values('volume_error', ascending=False), \n",
        "             'Volume Error > 100%')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show samples: Dimension error > 100%\n",
        "show_samples(sample_sets['dim_over_100'].sort_values('avg_dim_error', ascending=False), \n",
        "             'Dimension Error > 100%')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Select Experiment Samples"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Select balanced samples for experiment\n",
        "SAMPLE_SIZE = 25  # Per category\n",
        "\n",
        "experiment_samples = pd.concat([\n",
        "    sample_sets['weight_50_100'].sample(min(SAMPLE_SIZE, len(sample_sets['weight_50_100']))).assign(sample_group='weight_50_100'),\n",
        "    sample_sets['weight_over_100'].sample(min(SAMPLE_SIZE, len(sample_sets['weight_over_100']))).assign(sample_group='weight_over_100'),\n",
        "    sample_sets['volume_over_100'].sample(min(SAMPLE_SIZE, len(sample_sets['volume_over_100']))).assign(sample_group='volume_over_100'),\n",
        "    sample_sets['dim_over_100'].sample(min(SAMPLE_SIZE, len(sample_sets['dim_over_100']))).assign(sample_group='dim_over_100'),\n",
        "])\n",
        "\n",
        "# Remove duplicates (same order might appear in multiple categories)\n",
        "experiment_samples = experiment_samples.drop_duplicates(subset=['order_item_order_id'])\n",
        "\n",
        "print(f\"=== Experiment Sample Set ===\")\n",
        "print(f\"Total unique samples: {len(experiment_samples)}\")\n",
        "print(f\"\\nBy group:\")\n",
        "print(experiment_samples['sample_group'].value_counts())"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Export Samples"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Export to CSV\n",
        "from datetime import datetime\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "filename = f'experiment_samples_{timestamp}.csv'\n",
        "\n",
        "experiment_samples.to_csv(filename, index=False)\n",
        "print(f\"✅ Saved to {filename}\")\n",
        "\n",
        "# Download\n",
        "from google.colab import files\n",
        "files.download(filename)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Steps\n",
        "\n",
        "1. Use `weight_volume_experiment.ipynb` with these samples\n",
        "2. Re-estimate with improved prompt\n",
        "3. Compare new vs old error rates"
      ],
      "metadata": {}
    }
  ]
}
