{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Category-based Error Analysis\n",
    "\n",
    "This notebook analyzes weight/volume estimation errors grouped by category type:\n",
    "- **Meaningful**: Valid product categories with proper hierarchy (big > sub > detailed)\n",
    "- **Malformed**: Invalid categories (navigation, promotional, undefined, etc.)\n",
    "- **Empty**: Items with no category\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Set matplotlib style with fallback\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "except:\n",
    "    try:\n",
    "        plt.style.use('seaborn-whitegrid')\n",
    "    except:\n",
    "        pass  # Use default style\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "**For Google Colab**: Run this cell to upload the TSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab. Please upload '20260128_experiment_datasource.tsv'\")\n",
    "    uploaded = files.upload()\n",
    "    filename = list(uploaded.keys())[0]\n",
    "    print(f\"Uploaded: {filename}\")\n",
    "else:\n",
    "    filename = '20260128_experiment_datasource.tsv'\n",
    "    print(f\"Running locally. Using: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the experiment datasource\n",
    "df = pd.read_csv(filename, sep='\\t')\n",
    "print(f\"Data loaded successfully!\")\n",
    "print(f\"\\nTotal rows: {len(df):,}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Validation\n",
    "\n",
    "Check data completeness and calculate missing error values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data completeness\n",
    "print(\"DATA COMPLETENESS CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Actual values\n",
    "actual_cols = ['actual_weight', 'actual_d1', 'actual_d2', 'actual_d3', 'actual_volume_cm3']\n",
    "print(\"\\nActual Values (Ground Truth):\")\n",
    "for col in actual_cols:\n",
    "    if col in df.columns:\n",
    "        valid = df[col].notna().sum()\n",
    "        print(f\"  {col:20s}: {valid:6,} valid ({valid/len(df)*100:.1f}%)\")\n",
    "\n",
    "# AI estimated values\n",
    "ai_cols = ['ai_weight_kg', 'ai_width_cm', 'ai_depth_cm', 'ai_height_cm', 'ai_volume_cm3']\n",
    "print(\"\\nAI Estimated Values:\")\n",
    "for col in ai_cols:\n",
    "    if col in df.columns:\n",
    "        valid = df[col].notna().sum()\n",
    "        print(f\"  {col:20s}: {valid:6,} valid ({valid/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Error columns\n",
    "error_cols = ['weight_error', 'volume_error']\n",
    "print(\"\\nError Columns:\")\n",
    "for col in error_cols:\n",
    "    if col in df.columns:\n",
    "        valid = df[col].notna().sum()\n",
    "        print(f\"  {col:20s}: {valid:6,} valid ({valid/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing values\n",
    "print(\"CALCULATING MISSING VALUES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Calculate AI volume if missing\n",
    "if 'ai_volume_cm3' not in df.columns or df['ai_volume_cm3'].isna().all():\n",
    "    print(\"\\n1. Calculating ai_volume_cm3 from dimensions...\")\n",
    "    df['ai_volume_cm3'] = df['ai_width_cm'] * df['ai_depth_cm'] * df['ai_height_cm']\n",
    "    valid = df['ai_volume_cm3'].notna().sum()\n",
    "    print(f\"   Created ai_volume_cm3: {valid:,} valid values\")\n",
    "else:\n",
    "    # Fill missing values\n",
    "    missing = df['ai_volume_cm3'].isna().sum()\n",
    "    if missing > 0:\n",
    "        print(f\"\\n1. Filling {missing:,} missing ai_volume_cm3 values...\")\n",
    "        df['ai_volume_cm3'] = df['ai_volume_cm3'].fillna(\n",
    "            df['ai_width_cm'] * df['ai_depth_cm'] * df['ai_height_cm']\n",
    "        )\n",
    "\n",
    "# 2. Calculate volume_error if missing\n",
    "# Formula: (estimated - actual) / actual\n",
    "if 'volume_error' not in df.columns or df['volume_error'].isna().all():\n",
    "    print(\"\\n2. Calculating volume_error...\")\n",
    "    df['volume_error'] = (df['ai_volume_cm3'] - df['actual_volume_cm3']) / df['actual_volume_cm3']\n",
    "    valid = df['volume_error'].notna().sum()\n",
    "    print(f\"   Created volume_error: {valid:,} valid values\")\n",
    "else:\n",
    "    missing = df['volume_error'].isna().sum()\n",
    "    if missing > 0:\n",
    "        print(f\"\\n2. Filling {missing:,} missing volume_error values...\")\n",
    "        df['volume_error'] = df['volume_error'].fillna(\n",
    "            (df['ai_volume_cm3'] - df['actual_volume_cm3']) / df['actual_volume_cm3']\n",
    "        )\n",
    "\n",
    "# 3. Verify weight_error exists\n",
    "if 'weight_error' not in df.columns or df['weight_error'].isna().all():\n",
    "    print(\"\\n3. Calculating weight_error...\")\n",
    "    df['weight_error'] = (df['ai_weight_kg'] - df['actual_weight']) / df['actual_weight']\n",
    "    valid = df['weight_error'].notna().sum()\n",
    "    print(f\"   Created weight_error: {valid:,} valid values\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA AFTER CLEANING:\")\n",
    "print(f\"  weight_error valid: {df['weight_error'].notna().sum():,}\")\n",
    "print(f\"  volume_error valid: {df['volume_error'].notna().sum():,}\")\n",
    "print(f\"  Both valid:         {(df['weight_error'].notna() & df['volume_error'].notna()).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category Classification Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_empty_category(cat):\n",
    "    \"\"\"Check if category is empty\"\"\"\n",
    "    if pd.isna(cat):\n",
    "        return True\n",
    "    return not str(cat).strip()\n",
    "\n",
    "def is_malformed_category(cat):\n",
    "    \"\"\"Check if category is malformed (not a proper product hierarchy)\"\"\"\n",
    "    if is_empty_category(cat):\n",
    "        return False\n",
    "    \n",
    "    cat = str(cat)\n",
    "    \n",
    "    # 1. Contains 'undefined'\n",
    "    if 'undefined' in cat.lower():\n",
    "        return True\n",
    "    \n",
    "    # 2. Template/code patterns\n",
    "    template_patterns = ['_imgTag_', '_html_', '_name_', '{#item}', '{#title}', '{$', '_count}']\n",
    "    for p in template_patterns:\n",
    "        if p in cat:\n",
    "            return True\n",
    "    \n",
    "    # 3. Just a dash\n",
    "    if cat in ['-', '--', '---']:\n",
    "        return True\n",
    "    \n",
    "    # 4. Contains parentheses with numbers in navigation context\n",
    "    if re.search(r'\\(\\s*\\d+\\s*\\)', cat):\n",
    "        nav_indicators = ['ë¡œê·¸ì¸', 'íšŒì›ê°€ìž…', 'ìž¥ë°”êµ¬ë‹ˆ', 'ì¹´íŠ¸', 'cart', 'Cart', 'CART', \n",
    "                          'ë§ˆì´íŽ˜ì´ì§€', 'Q&A', 'FAQ', 'REVIEW', 'ìƒí’ˆí›„ê¸°', 'ìƒí’ˆë¬¸ì˜', \n",
    "                          'ìƒì„¸ì •ë³´', 'ê¸°ë³¸ì •ë³´', 'DETAIL', 'GUIDE']\n",
    "        for indicator in nav_indicators:\n",
    "            if indicator in cat:\n",
    "                return True\n",
    "    \n",
    "    # 5. Navigation/UI/Account keywords\n",
    "    nav_only_patterns = [\n",
    "        'ë¡œê·¸ì¸', 'íšŒì›ê°€ìž…', 'ë§ˆì´íŽ˜ì´ì§€', 'ë§ˆì´ì‡¼í•‘', 'ì£¼ë¬¸ì¡°íšŒ', \n",
    "        'ì¹´íŠ¸ ', 'cart ', 'Cart ', 'LOGIN', 'JOIN', 'CART', 'MYPAGE', 'MY ACCOUNT',\n",
    "        'MY PAGE', 'ORDER CHECK', 'YOUR BAG', 'Sign In', 'Sign Up',\n",
    "        'ë°”ë¡œê°€ê¸°', 'ë‹«ê¸° >', 'ë’¤ë¡œê°€ê¸°', 'ë’¤ë¡œ >', 'ì´ì „ íŽ˜ì´ì§€',\n",
    "        'ì¸ê¸° ê²€ìƒ‰ì–´', 'ìµœê·¼ ê²€ìƒ‰ì–´', 'Press Enter',\n",
    "        'Q&A', 'FAQ', 'NOTICE', 'ë¦¬ë·°ì´ë²¤íŠ¸', 'ê³µì§€ì‚¬í•­', 'íŒë§¤ìž ì •ë³´',\n",
    "        'ìƒì„¸ì„¤ëª… >', 'ìƒì„¸ì •ë³´ >', 'ê¸°ë³¸ì •ë³´', 'êµ¬ë§¤ì•ˆë‚´', 'DETAIL >', 'GUIDE >',\n",
    "        'INFO >', 'SIZE CHART', 'WASHING', 'DELIVERY', 'EXCHANGE', 'RETURN',\n",
    "        'ë¹„íšŒì›', 'ì•„ì´ë”” ì°¾ê¸°', 'ë¹„ë°€ë²ˆí˜¸ ì°¾ê¸°',\n",
    "        'SHARE', 'URL >', 'kakaostory', 'kakao >', 'instagram', 'INSTAGRAM'\n",
    "    ]\n",
    "    for kw in nav_only_patterns:\n",
    "        if kw in cat:\n",
    "            return True\n",
    "    \n",
    "    # 6. Promotional/marketing patterns\n",
    "    promo_patterns = [\n",
    "        'ì¿ í°', 'í• ì¸ì¿ í°', ' SALE', ' sale', 'ë¸”í”„', 'BLACK FRIDAY',\n",
    "        'DROP', 'OPEN >', 'PRE-ORDER', 'ðŸ“¢', 'â°', 'â­', 'ðŸ’', 'ðŸ“', 'â›„', 'â™¥', 'âœ¦',\n",
    "        'ë¬´ë£Œë°°ì†¡', 'ì›°ì»´', 'ë©¤ë²„ì‹­', 'MEMBERSHIP', 'Membership',\n",
    "        'ì‹ ê·œíšŒì›', 'ê°€ìž… ì‹œ', 'ì¶”ê°€ ì‹œ', '% off', \n",
    "        '~90%', '~70%', '~60%', '~50%', '~40%', '~30%', '~20%', '~10%',\n",
    "        'COLLECTION. SEE MORE', 'SEE MORE', 'SHOP NOW',\n",
    "        '100ì›', '3,000ì›', 'PICKâ™¥', 'PICK >'\n",
    "    ]\n",
    "    for p in promo_patterns:\n",
    "        if p in cat:\n",
    "            return True\n",
    "    \n",
    "    # 7. Shipping/payment context\n",
    "    if any(x in cat for x in ['ë°°ì†¡ ë°©ë²•', 'ë°°ì†¡ë°©ë²•', 'ë°°ì†¡ ì§€ì—­', 'ë°°ì†¡ ë¹„ìš©', \n",
    "                               'ë°°ì†¡ ì•ˆë‚´', 'ê²°ì œ ì‹œ', 'ë¬´í†µìž¥', 'ìž…ê¸ˆ í™•ì¸',\n",
    "                               'ë°˜í’ˆ/êµí™˜', 'êµí™˜/ë°˜í’ˆ', 'a/sì•ˆë‚´', 'A/Sì•ˆë‚´', 'ìƒí’ˆìƒì„¸ì°¸ì¡°',\n",
    "                               'ì—°ê´€ìƒí’ˆ', 'ìˆ˜ìˆ˜ë£Œ', 'ê±°ì£¼', 'ì…€ëŸ¬']):\n",
    "        return True\n",
    "    \n",
    "    # 8. Single/incomplete categories ending with >\n",
    "    parts = [p.strip() for p in cat.split('>')]\n",
    "    non_empty_parts = [p for p in parts if p]\n",
    "    \n",
    "    if cat.strip().endswith('>'):\n",
    "        if len(non_empty_parts) <= 1:\n",
    "            return True\n",
    "        if len(non_empty_parts) == 2 and non_empty_parts[0] in [\n",
    "            'í™ˆ', 'HOME', 'Home', 'home', 'ê¸°íƒ€', 'Detail', 'TOP', 'HEADWEAR',\n",
    "            'PRODUCTS', 'MOMENTEL', 'NICHES', 'YOOKIESHOT', 'BRAND', 'Menu',\n",
    "            'close', 'more', 'Today', 'o w o n', 'PETER AND WENDY'\n",
    "        ]:\n",
    "            return True\n",
    "    \n",
    "    # 9. Specific garbage patterns\n",
    "    garbage_patterns = [\n",
    "        'ourcabinet', 'ì†Œë“ê³µì œ >', 'AW24 New Arrival SHOP NOW',\n",
    "        '24FW NEW COLLECTION >', '365ì¼ì´ˆíŠ¹ê°€', 'ìŠˆí”¼ê² ê³µì§€ì‚¬í•­',\n",
    "        'ë“€ì´ ë©¤ë²„ì‹­', 'ì§€ê³µê°€ëŠ¥í•œíšŒì›ìƒµ', 'ë…¸ë²¨ë¬¸í•™ìƒ >', 'SALEìƒµ',\n",
    "        '# ê²€ìƒ‰ì–´', 'COMPANY > AGREEMENT', 'shop > story > sale',\n",
    "        '1 . ì»¬ë¦¬ì§€', 'ì „ì²´ìƒí’ˆ > ì‚°ë¦¬ì˜¤', 'About > Archive > Wholesale',\n",
    "        'ì¿ íŒ¡ í™ˆ > ì¿ íŒ¡ìˆ˜ìž…', 'â˜…ì£¼ë§íŠ¹ê°€'\n",
    "    ]\n",
    "    for p in garbage_patterns:\n",
    "        if p in cat:\n",
    "            return True\n",
    "    \n",
    "    # 10. Very short standalone tokens\n",
    "    if cat in ['HOME', 'SHOP', 'ALL', 'BEST', 'NEW', 'TOP', 'OUTER', 'BOTTOM', \n",
    "               'HEADWEAR', 'PRODUCTS', 'BRAND', 'GOODS', 'ACC']:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def classify_category(cat):\n",
    "    \"\"\"Classify category into: empty, malformed, or meaningful\"\"\"\n",
    "    if is_empty_category(cat):\n",
    "        return 'empty'\n",
    "    elif is_malformed_category(cat):\n",
    "        return 'malformed'\n",
    "    else:\n",
    "        return 'meaningful'\n",
    "\n",
    "print(\"Classification functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify each row\n",
    "df['category_type'] = df['category'].apply(classify_category)\n",
    "\n",
    "# Summary\n",
    "category_counts = df['category_type'].value_counts()\n",
    "print(\"Category Type Distribution:\")\n",
    "print(\"=\" * 40)\n",
    "for cat_type in ['meaningful', 'malformed', 'empty']:\n",
    "    count = category_counts.get(cat_type, 0)\n",
    "    pct = count / len(df) * 100\n",
    "    print(f\"{cat_type:12s}: {count:6,} ({pct:5.1f}%)\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"{'Total':12s}: {len(df):6,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Error Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set standard column names\n",
    "WEIGHT_ERROR = 'weight_error'\n",
    "VOLUME_ERROR = 'volume_error'\n",
    "\n",
    "# Filter rows with valid errors (both weight and volume)\n",
    "df_valid = df.dropna(subset=[WEIGHT_ERROR, VOLUME_ERROR]).copy()\n",
    "print(f\"Rows with valid weight_error AND volume_error: {len(df_valid):,}\")\n",
    "\n",
    "# Create combined error (average of absolute errors)\n",
    "df_valid['combined_error'] = (df_valid[WEIGHT_ERROR].abs() + df_valid[VOLUME_ERROR].abs()) / 2\n",
    "\n",
    "# Show distribution by category type\n",
    "print(\"\\nValid rows by category type:\")\n",
    "print(df_valid['category_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Statistics by Category Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_error_stats(data):\n",
    "    \"\"\"Calculate error statistics for a series\"\"\"\n",
    "    data = data.dropna()\n",
    "    if len(data) == 0:\n",
    "        return pd.Series({\n",
    "            'count': 0, 'min': np.nan, 'max': np.nan, \n",
    "            'median': np.nan, 'mean': np.nan, 'std': np.nan,\n",
    "            'q25': np.nan, 'q75': np.nan\n",
    "        })\n",
    "    return pd.Series({\n",
    "        'count': len(data),\n",
    "        'min': data.min(),\n",
    "        'max': data.max(),\n",
    "        'median': data.median(),\n",
    "        'mean': data.mean(),\n",
    "        'std': data.std(),\n",
    "        'q25': data.quantile(0.25),\n",
    "        'q75': data.quantile(0.75)\n",
    "    })\n",
    "\n",
    "# Calculate stats for each category type and error type\n",
    "error_types = [WEIGHT_ERROR, VOLUME_ERROR, 'combined_error']\n",
    "cat_order = ['meaningful', 'malformed', 'empty']\n",
    "\n",
    "for error_type in error_types:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{error_type.upper()} Statistics by Category Type\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    stats_list = []\n",
    "    for cat_type in cat_order:\n",
    "        subset = df_valid[df_valid['category_type'] == cat_type][error_type]\n",
    "        stats = calc_error_stats(subset)\n",
    "        stats.name = cat_type\n",
    "        stats_list.append(stats)\n",
    "    \n",
    "    stats_df = pd.DataFrame(stats_list)\n",
    "    print(stats_df.round(4).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Error Distribution by Category Type\n",
    "\n",
    "### Box Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color palette for category types\n",
    "colors = {'meaningful': '#2ecc71', 'malformed': '#e74c3c', 'empty': '#95a5a6'}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "titles = ['Weight Error', 'Volume Error', 'Combined Error']\n",
    "\n",
    "for ax, error_type, title in zip(axes, error_types, titles):\n",
    "    # Prepare data for box plot\n",
    "    data_to_plot = []\n",
    "    labels = []\n",
    "    for cat_type in cat_order:\n",
    "        subset = df_valid[df_valid['category_type'] == cat_type][error_type].dropna()\n",
    "        # Clip extreme values for better visualization\n",
    "        subset_clipped = subset.clip(-5, 5)\n",
    "        data_to_plot.append(subset_clipped.values)\n",
    "        labels.append(f\"{cat_type}\\n(n={len(subset):,})\")\n",
    "    \n",
    "    # Box plot\n",
    "    bp = ax.boxplot(data_to_plot, labels=labels, patch_artist=True,\n",
    "                    showfliers=False, widths=0.6)\n",
    "    \n",
    "    # Color the boxes\n",
    "    for patch, cat_type in zip(bp['boxes'], cat_order):\n",
    "        patch.set_facecolor(colors[cat_type])\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    # Add horizontal line at 0\n",
    "    ax.axhline(y=0, color='black', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    \n",
    "    # Add median annotations\n",
    "    for i, cat_type in enumerate(cat_order):\n",
    "        subset = df_valid[df_valid['category_type'] == cat_type][error_type]\n",
    "        if len(subset) > 0:\n",
    "            median = subset.median()\n",
    "            ax.annotate(f'med: {median:.3f}', xy=(i+1, median), \n",
    "                        xytext=(i+1.3, median), fontsize=9,\n",
    "                        ha='left', va='center')\n",
    "    \n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Error (clipped to [-5, 5])')\n",
    "    ax.set_ylim(-5.5, 5.5)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Error Distribution by Category Type', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('category_error_boxplot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\nBox plot saved as 'category_error_boxplot.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram and CDF Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "for col, (error_type, title) in enumerate(zip(error_types, titles)):\n",
    "    # Top row: full distribution (clipped)\n",
    "    ax = axes[0, col]\n",
    "    for cat_type in cat_order:\n",
    "        subset = df_valid[df_valid['category_type'] == cat_type][error_type].dropna()\n",
    "        if len(subset) > 0:\n",
    "            subset_clipped = subset.clip(-3, 3)\n",
    "            ax.hist(subset_clipped.values, bins=50, alpha=0.5, label=cat_type, \n",
    "                    color=colors[cat_type], density=True)\n",
    "    ax.axvline(x=0, color='black', linestyle='--', alpha=0.7)\n",
    "    ax.set_title(f'{title} Distribution', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Error (clipped to [-3, 3])')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "    ax.set_xlim(-3.5, 3.5)\n",
    "    \n",
    "    # Bottom row: cumulative distribution\n",
    "    ax = axes[1, col]\n",
    "    for cat_type in cat_order:\n",
    "        subset = df_valid[df_valid['category_type'] == cat_type][error_type].abs().dropna()\n",
    "        if len(subset) > 0:\n",
    "            sorted_data = np.sort(subset.values)\n",
    "            cumulative = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "            ax.plot(sorted_data, cumulative, label=cat_type, color=colors[cat_type], linewidth=2)\n",
    "    ax.axvline(x=0.5, color='gray', linestyle=':', alpha=0.7, label='50% error')\n",
    "    ax.axvline(x=1.0, color='gray', linestyle='--', alpha=0.7, label='100% error')\n",
    "    ax.set_title(f'{title} CDF (Absolute)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Absolute Error')\n",
    "    ax.set_ylabel('Cumulative Probability')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_xlim(0, 3)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Error Distribution Comparison by Category Type', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('category_error_histogram.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\nHistogram saved as 'category_error_histogram.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table\n",
    "summary_data = []\n",
    "\n",
    "for cat_type in cat_order:\n",
    "    subset = df_valid[df_valid['category_type'] == cat_type]\n",
    "    \n",
    "    if len(subset) > 0:\n",
    "        row = {\n",
    "            'Category Type': cat_type,\n",
    "            'Count': len(subset),\n",
    "            'Weight Min': subset[WEIGHT_ERROR].min(),\n",
    "            'Weight Max': subset[WEIGHT_ERROR].max(),\n",
    "            'Weight Median': subset[WEIGHT_ERROR].median(),\n",
    "            'Volume Min': subset[VOLUME_ERROR].min(),\n",
    "            'Volume Max': subset[VOLUME_ERROR].max(),\n",
    "            'Volume Median': subset[VOLUME_ERROR].median(),\n",
    "            'Combined Median': subset['combined_error'].median()\n",
    "        }\n",
    "    else:\n",
    "        row = {\n",
    "            'Category Type': cat_type,\n",
    "            'Count': 0,\n",
    "            'Weight Min': np.nan, 'Weight Max': np.nan, 'Weight Median': np.nan,\n",
    "            'Volume Min': np.nan, 'Volume Max': np.nan, 'Volume Median': np.nan,\n",
    "            'Combined Median': np.nan\n",
    "        }\n",
    "    summary_data.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nError Summary by Category Type:\")\n",
    "print(\"=\" * 100)\n",
    "print(summary_df.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Summary: Min, Median, Max by Category Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "x = np.arange(len(cat_order))\n",
    "width = 0.25\n",
    "\n",
    "# Weight Error\n",
    "ax = axes[0]\n",
    "mins = [df_valid[df_valid['category_type']==ct][WEIGHT_ERROR].min() if len(df_valid[df_valid['category_type']==ct]) > 0 else 0 for ct in cat_order]\n",
    "medians = [df_valid[df_valid['category_type']==ct][WEIGHT_ERROR].median() if len(df_valid[df_valid['category_type']==ct]) > 0 else 0 for ct in cat_order]\n",
    "maxs = [df_valid[df_valid['category_type']==ct][WEIGHT_ERROR].max() if len(df_valid[df_valid['category_type']==ct]) > 0 else 0 for ct in cat_order]\n",
    "\n",
    "bars1 = ax.bar(x - width, mins, width, label='Min', color='#3498db', alpha=0.8)\n",
    "bars2 = ax.bar(x, medians, width, label='Median', color='#2ecc71', alpha=0.8)\n",
    "bars3 = ax.bar(x + width, maxs, width, label='Max', color='#e74c3c', alpha=0.8)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars1, mins):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{val:.2f}', \n",
    "            ha='center', va='bottom' if val >= 0 else 'top', fontsize=8)\n",
    "for bar, val in zip(bars2, medians):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{val:.2f}', \n",
    "            ha='center', va='bottom' if val >= 0 else 'top', fontsize=8)\n",
    "for bar, val in zip(bars3, maxs):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{val:.2f}', \n",
    "            ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "ax.set_ylabel('Weight Error')\n",
    "ax.set_title('Weight Error: Min, Median, Max by Category Type', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(cat_order)\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Volume Error\n",
    "ax = axes[1]\n",
    "mins = [df_valid[df_valid['category_type']==ct][VOLUME_ERROR].min() if len(df_valid[df_valid['category_type']==ct]) > 0 else 0 for ct in cat_order]\n",
    "medians = [df_valid[df_valid['category_type']==ct][VOLUME_ERROR].median() if len(df_valid[df_valid['category_type']==ct]) > 0 else 0 for ct in cat_order]\n",
    "maxs = [df_valid[df_valid['category_type']==ct][VOLUME_ERROR].max() if len(df_valid[df_valid['category_type']==ct]) > 0 else 0 for ct in cat_order]\n",
    "\n",
    "bars1 = ax.bar(x - width, mins, width, label='Min', color='#3498db', alpha=0.8)\n",
    "bars2 = ax.bar(x, medians, width, label='Median', color='#2ecc71', alpha=0.8)\n",
    "bars3 = ax.bar(x + width, maxs, width, label='Max', color='#e74c3c', alpha=0.8)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars1, mins):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{val:.2f}', \n",
    "            ha='center', va='bottom' if val >= 0 else 'top', fontsize=8)\n",
    "for bar, val in zip(bars2, medians):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{val:.2f}', \n",
    "            ha='center', va='bottom' if val >= 0 else 'top', fontsize=8)\n",
    "for bar, val in zip(bars3, maxs):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{val:.2f}', \n",
    "            ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "ax.set_ylabel('Volume Error')\n",
    "ax.set_title('Volume Error: Min, Median, Max by Category Type', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(cat_order)\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('category_error_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\nSummary chart saved as 'category_error_summary.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Datasources by Category Type\n",
    "\n",
    "Create separate TSV files for each category type for future experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate and save\n",
    "output_files = {}\n",
    "\n",
    "for cat_type in cat_order:\n",
    "    subset = df[df['category_type'] == cat_type].copy()\n",
    "    filename = f'datasource_{cat_type}.tsv'\n",
    "    subset.to_csv(filename, sep='\\t', index=False)\n",
    "    output_files[cat_type] = filename\n",
    "    print(f\"Saved {cat_type:12s}: {len(subset):6,} rows -> {filename}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Files created for experiment:\")\n",
    "for cat_type, filename in output_files.items():\n",
    "    print(f\"  - {filename}\")\n",
    "\n",
    "# Download files if in Colab\n",
    "if IN_COLAB:\n",
    "    print(\"\\nDownloading files...\")\n",
    "    for filename in output_files.values():\n",
    "        files.download(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Analysis: Error by Meaningful Subcategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For meaningful categories, analyze top-level categories\n",
    "df_meaningful = df_valid[df_valid['category_type'] == 'meaningful'].copy()\n",
    "\n",
    "if len(df_meaningful) > 0:\n",
    "    def get_top_category(cat):\n",
    "        \"\"\"Extract top-level category\"\"\"\n",
    "        if pd.isna(cat):\n",
    "            return 'Unknown'\n",
    "        cat = str(cat)\n",
    "        # Split by > or /\n",
    "        if '>' in cat:\n",
    "            return cat.split('>')[0].strip()\n",
    "        elif '/' in cat:\n",
    "            return cat.split('/')[0].strip()\n",
    "        return cat.strip()\n",
    "\n",
    "    df_meaningful['top_category'] = df_meaningful['category'].apply(get_top_category)\n",
    "\n",
    "    # Top 15 categories by count\n",
    "    top_cats = df_meaningful['top_category'].value_counts().head(15).index.tolist()\n",
    "\n",
    "    # Calculate stats for top categories\n",
    "    cat_stats = []\n",
    "    for cat in top_cats:\n",
    "        subset = df_meaningful[df_meaningful['top_category'] == cat]\n",
    "        cat_stats.append({\n",
    "            'Top Category': cat,\n",
    "            'Count': len(subset),\n",
    "            'Weight Median': subset[WEIGHT_ERROR].median(),\n",
    "            'Volume Median': subset[VOLUME_ERROR].median(),\n",
    "            'Combined Median': subset['combined_error'].median()\n",
    "        })\n",
    "\n",
    "    cat_stats_df = pd.DataFrame(cat_stats)\n",
    "    print(\"\\nTop 15 Categories (Meaningful) - Error Statistics:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(cat_stats_df.round(4).to_string(index=False))\n",
    "else:\n",
    "    print(\"No meaningful categories with valid error data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top categories\n",
    "if len(df_meaningful) > 0 and len(top_cats) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    x = np.arange(len(top_cats))\n",
    "    width = 0.35\n",
    "\n",
    "    weight_medians = [df_meaningful[df_meaningful['top_category']==cat][WEIGHT_ERROR].median() for cat in top_cats]\n",
    "    volume_medians = [df_meaningful[df_meaningful['top_category']==cat][VOLUME_ERROR].median() for cat in top_cats]\n",
    "\n",
    "    bars1 = ax.bar(x - width/2, weight_medians, width, label='Weight Error', color='#3498db', alpha=0.8)\n",
    "    bars2 = ax.bar(x + width/2, volume_medians, width, label='Volume Error', color='#e74c3c', alpha=0.8)\n",
    "\n",
    "    ax.set_ylabel('Median Error')\n",
    "    ax.set_title('Median Error by Top Category (Meaningful Categories Only)', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(top_cats, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('top_category_error.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\nTop category chart saved as 'top_category_error.png'\")\n",
    "else:\n",
    "    print(\"Skipping top category visualization - no data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings:\n",
    "1. **Category Distribution**: Meaningful categories have the majority of data\n",
    "2. **Error Comparison**: Compare median errors across category types\n",
    "3. **Separated Datasources**: Three files created for targeted experiments\n",
    "\n",
    "### Output Files:\n",
    "- `datasource_meaningful.tsv` - Items with valid product categories\n",
    "- `datasource_malformed.tsv` - Items with invalid/navigation categories  \n",
    "- `datasource_empty.tsv` - Items with no category\n",
    "- `category_error_boxplot.png` - Box plot visualization\n",
    "- `category_error_histogram.png` - Histogram and CDF\n",
    "- `category_error_summary.png` - Min/Median/Max bar chart\n",
    "- `top_category_error.png` - Top meaningful categories analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
