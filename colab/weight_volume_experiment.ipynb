{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Weight & Volume Estimation Experiment\n",
        "\n",
        "This notebook experiments with AI-based weight/volume estimation using:\n",
        "- **BigQuery** for data\n",
        "- **OpenAI GPT-4o-mini** for estimation\n",
        "- **Visualization** for error analysis\n",
        "\n",
        "## Experiment Approaches\n",
        "1. **Big Error Items** - Focus on items with high estimation error\n",
        "2. **Uniformity Check** - Same products, multiple orders"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup & Authentication"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q google-cloud-bigquery openai pandas matplotlib seaborn tqdm"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate with Google Cloud\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "print(\"✅ Google Cloud authenticated\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup BigQuery client\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "\n",
        "PROJECT_ID = \"sazoshop\"  # Your GCP project ID\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "print(f\"✅ BigQuery client ready (project: {PROJECT_ID})\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup OpenAI client\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Option 1: Use Colab Secrets (recommended)\n",
        "# Go to: Runtime > Secrets > Add OPENAI_API_KEY\n",
        "try:\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "except:\n",
        "    # Option 2: Manual input (less secure)\n",
        "    OPENAI_API_KEY = input(\"Enter your OpenAI API key: \")\n",
        "\n",
        "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "print(\"✅ OpenAI client ready\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load Data from BigQuery"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Query 1: High Error Items (Weight)\n",
        "# Items with highest weight estimation error\n",
        "\n",
        "QUERY_HIGH_ERROR_WEIGHT = \"\"\"\n",
        "SELECT\n",
        "  oid.order_item_order_id,\n",
        "  oid.order_item_title_origin AS product_title,\n",
        "  oid.order_item_product_version_info_category AS category,\n",
        "  ARRAY_TO_STRING(oid.order_item_product_version_thumbnail_urls, '|') AS thumbnail_urls,\n",
        "  \n",
        "  -- Actual vs Estimated\n",
        "  kse.actual_weight AS actual_weight_kg,\n",
        "  SAFE_CAST(JSON_EXTRACT_SCALAR(oid.order_item_product_version_extra, '$.weight') AS FLOAT64) AS ai_weight_kg,\n",
        "  \n",
        "  -- Error rate\n",
        "  ABS(SAFE_CAST(JSON_EXTRACT_SCALAR(oid.order_item_product_version_extra, '$.weight') AS FLOAT64) - kse.actual_weight) / kse.actual_weight AS weight_error_rate,\n",
        "  \n",
        "  kse.dimensions AS actual_dimensions\n",
        "\n",
        "FROM `sazoshop.firestore_snapshot.v2_order_items` oid\n",
        "INNER JOIN `sazoshop.firestore_collection.v2_kse_cost` kse \n",
        "  ON oid.order_item_order_id = kse.order_id\n",
        "\n",
        "WHERE\n",
        "  kse.actual_weight > 0 AND kse.actual_weight < 50\n",
        "  AND kse.dimensions IS NOT NULL\n",
        "  AND REGEXP_CONTAINS(kse.dimensions, r'^\\\\d+\\\\.?\\\\d*x\\\\d+\\\\.?\\\\d*x\\\\d+\\\\.?\\\\d*$')\n",
        "  AND JSON_EXTRACT_SCALAR(oid.order_item_product_version_extra, '$.weight') IS NOT NULL\n",
        "  AND oid.order_item_title_origin IS NOT NULL\n",
        "\n",
        "ORDER BY weight_error_rate DESC\n",
        "LIMIT 100\n",
        "\"\"\"\n",
        "\n",
        "df = client.query(QUERY_HIGH_ERROR_WEIGHT).to_dataframe()\n",
        "print(f\"✅ Loaded {len(df)} high-error items\")\n",
        "df.head()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick data exploration\n",
        "print(\"=== Data Summary ===\")\n",
        "print(f\"Total items: {len(df)}\")\n",
        "print(f\"\\nWeight Error Rate:\")\n",
        "print(df['weight_error_rate'].describe())\n",
        "print(f\"\\nActual Weight (kg):\")\n",
        "print(df['actual_weight_kg'].describe())"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display sample images\n",
        "from IPython.display import display, HTML, Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "def show_product_sample(row):\n",
        "    \"\"\"Display product info with image\"\"\"\n",
        "    urls = row['thumbnail_urls'].split('|') if row['thumbnail_urls'] else []\n",
        "    img_html = \"\"\n",
        "    if urls:\n",
        "        img_html = f'<img src=\"{urls[0]}\" style=\"max-width:150px; max-height:150px;\">'\n",
        "    \n",
        "    html = f\"\"\"\n",
        "    <div style=\"border:1px solid #ccc; padding:10px; margin:5px; display:inline-block; width:300px;\">\n",
        "        {img_html}\n",
        "        <p><b>{row['product_title'][:50]}...</b></p>\n",
        "        <p>Category: {row['category']}</p>\n",
        "        <p>Actual: {row['actual_weight_kg']:.2f} kg | AI: {row['ai_weight_kg']:.2f} kg</p>\n",
        "        <p>Error: {row['weight_error_rate']*100:.1f}%</p>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    return html\n",
        "\n",
        "# Show top 6 high-error items\n",
        "html_output = \"<h3>Top High-Error Items</h3><div>\"\n",
        "for _, row in df.head(6).iterrows():\n",
        "    html_output += show_product_sample(row)\n",
        "html_output += \"</div>\"\n",
        "display(HTML(html_output))"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Define Estimation Prompt"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# System prompt (from prompts/weight-volume.system.txt)\n",
        "SYSTEM_PROMPT = \"\"\"You are a bot responsible for estimating the volume and weight of a product based on its data, focusing on packed volume estimation for items that can be folded, stacked, or compressed.\n",
        "\n",
        "# Instructions\n",
        "You will receive the following product data from the user:\n",
        "- Title\n",
        "- Category\n",
        "- Image (if provided)\n",
        "\n",
        "Use the text data and the image (if provided) to generate the most appropriate volume and weight for the product entered by the user.\n",
        "\n",
        "# Estimation Process\n",
        "Estimate the volume and weight based on the product's category, description, and image analysis. For items that can be folded, stacked, or compressed (e.g., clothing, towels, bedding, bags), estimate a reduced packed volume based on typical packing practices.\n",
        "\n",
        "- **Clothing Items:** Use a standard packed volume of 3x15x15 cm and weight of 0.5 kg.\n",
        "- **Foldable Items (e.g., towels, bags):** Estimate packed volume based on 50% reduction from their actual size if details indicate foldability or compressibility.\n",
        "- **Stackable Items (e.g., boxes, storage containers):** Estimate packed volume considering stacking with a 30% volume reduction.\n",
        "- **Non-Foldable/Non-Compressible Items:** Use actual dimensions without reduction.\n",
        "- **Papers:** If the product can be reduced in volume by folding or rolling, take that into account and estimate the volume.\n",
        "\n",
        "# Output Format\n",
        "Provide the estimated weight and volume (including packed volume for foldable/compressible items) in JSON format. The output should include the volume in cm (width, length, depth), the weight in kg, and a brief reason for the estimate.\n",
        "\n",
        "# Cautions\n",
        "!Do not describe the reasoning process.\n",
        "!Keep in mind that foldable or stackable sizes should be standardized for packaging.\n",
        "!Answers should be in JSON format only.\n",
        "\n",
        "# Example Output\n",
        "{\n",
        "  \"volume\": \"20x40x2\",\n",
        "  \"packed_volume\": \"10x20x2\",\n",
        "  \"weight\": 0.6,\n",
        "  \"reason\": \"Based on foldability and image analysis.\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "print(\"✅ System prompt loaded\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Estimation Function"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "\n",
        "def estimate_weight_volume(row, use_image=True):\n",
        "    \"\"\"\n",
        "    Call OpenAI API to estimate weight and volume.\n",
        "    Supports multimodal (text + image).\n",
        "    \"\"\"\n",
        "    # Build user message content\n",
        "    user_content = []\n",
        "    \n",
        "    # Text part\n",
        "    text_prompt = f\"\"\"Title: {row['product_title']}\n",
        "Category: {row['category']}\"\"\"\n",
        "    user_content.append({\"type\": \"text\", \"text\": text_prompt})\n",
        "    \n",
        "    # Image part (if available and enabled)\n",
        "    if use_image and row['thumbnail_urls']:\n",
        "        image_url = row['thumbnail_urls'].split('|')[0]  # First image\n",
        "        user_content.append({\n",
        "            \"type\": \"image_url\",\n",
        "            \"image_url\": {\"url\": image_url}\n",
        "        })\n",
        "    \n",
        "    try:\n",
        "        response = openai_client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": user_content}\n",
        "            ],\n",
        "            temperature=0.01,\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "        \n",
        "        result = json.loads(response.choices[0].message.content)\n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"weight\": result.get(\"weight\"),\n",
        "            \"volume\": result.get(\"volume\"),\n",
        "            \"packed_volume\": result.get(\"packed_volume\"),\n",
        "            \"reason\": result.get(\"reason\")\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "# Test with first row\n",
        "test_result = estimate_weight_volume(df.iloc[0])\n",
        "print(\"Test result:\")\n",
        "print(json.dumps(test_result, indent=2))"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Run Batch Estimation"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Limit for testing (set to len(df) for full run)\n",
        "LIMIT = 20\n",
        "\n",
        "results = []\n",
        "for idx, row in tqdm(df.head(LIMIT).iterrows(), total=LIMIT, desc=\"Estimating\"):\n",
        "    result = estimate_weight_volume(row, use_image=True)\n",
        "    result['order_id'] = row['order_item_order_id']\n",
        "    result['actual_weight_kg'] = row['actual_weight_kg']\n",
        "    result['old_ai_weight_kg'] = row['ai_weight_kg']\n",
        "    result['actual_dimensions'] = row['actual_dimensions']\n",
        "    results.append(result)\n",
        "    time.sleep(0.5)  # Rate limiting\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(f\"\\n✅ Completed {len(results_df)} estimations\")\n",
        "results_df.head()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Calculate Error Metrics"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter successful results\n",
        "success_df = results_df[results_df['success'] == True].copy()\n",
        "print(f\"Successful: {len(success_df)} / {len(results_df)}\")\n",
        "\n",
        "# Calculate new weight (handle None)\n",
        "success_df['new_weight_kg'] = pd.to_numeric(success_df['weight'], errors='coerce')\n",
        "\n",
        "# Calculate error rates\n",
        "success_df['old_error_rate'] = abs(success_df['old_ai_weight_kg'] - success_df['actual_weight_kg']) / success_df['actual_weight_kg']\n",
        "success_df['new_error_rate'] = abs(success_df['new_weight_kg'] - success_df['actual_weight_kg']) / success_df['actual_weight_kg']\n",
        "\n",
        "# Summary\n",
        "print(\"\\n=== Error Rate Comparison ===\")\n",
        "print(f\"Old AI Mean Error: {success_df['old_error_rate'].mean()*100:.1f}%\")\n",
        "print(f\"New AI Mean Error: {success_df['new_error_rate'].mean()*100:.1f}%\")\n",
        "print(f\"\\nImprovement: {(success_df['old_error_rate'].mean() - success_df['new_error_rate'].mean())*100:.1f}%\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Visualization"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# 1. Error Rate Distribution\n",
        "ax1 = axes[0]\n",
        "sns.kdeplot(success_df['old_error_rate'], label='Old AI', color='steelblue', fill=True, ax=ax1)\n",
        "sns.kdeplot(success_df['new_error_rate'], label='New AI', color='tomato', fill=True, ax=ax1)\n",
        "ax1.set_title('Error Rate Distribution')\n",
        "ax1.set_xlabel('Error Rate')\n",
        "ax1.legend()\n",
        "\n",
        "# 2. Predicted vs Actual (New)\n",
        "ax2 = axes[1]\n",
        "ax2.scatter(success_df['actual_weight_kg'], success_df['new_weight_kg'], alpha=0.6, label='New AI')\n",
        "ax2.scatter(success_df['actual_weight_kg'], success_df['old_ai_weight_kg'], alpha=0.4, label='Old AI', marker='x')\n",
        "max_val = max(success_df['actual_weight_kg'].max(), success_df['new_weight_kg'].max())\n",
        "ax2.plot([0, max_val], [0, max_val], '--', color='gray', label='Perfect')\n",
        "ax2.set_title('Predicted vs Actual Weight')\n",
        "ax2.set_xlabel('Actual Weight (kg)')\n",
        "ax2.set_ylabel('Predicted Weight (kg)')\n",
        "ax2.legend()\n",
        "\n",
        "# 3. Improvement per item\n",
        "ax3 = axes[2]\n",
        "improvement = success_df['old_error_rate'] - success_df['new_error_rate']\n",
        "colors = ['green' if x > 0 else 'red' for x in improvement]\n",
        "ax3.bar(range(len(improvement)), improvement, color=colors)\n",
        "ax3.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "ax3.set_title('Improvement per Item (+ = better)')\n",
        "ax3.set_xlabel('Item Index')\n",
        "ax3.set_ylabel('Error Reduction')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detailed comparison table\n",
        "comparison_df = success_df[[\n",
        "    'order_id', 'actual_weight_kg', 'old_ai_weight_kg', 'new_weight_kg',\n",
        "    'old_error_rate', 'new_error_rate', 'reason'\n",
        "]].copy()\n",
        "\n",
        "comparison_df['improved'] = comparison_df['new_error_rate'] < comparison_df['old_error_rate']\n",
        "comparison_df = comparison_df.round(3)\n",
        "\n",
        "print(f\"\\nImproved: {comparison_df['improved'].sum()} / {len(comparison_df)} ({comparison_df['improved'].mean()*100:.0f}%)\")\n",
        "comparison_df"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Export Results"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to CSV\n",
        "from datetime import datetime\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "filename = f'experiment_results_{timestamp}.csv'\n",
        "\n",
        "results_df.to_csv(filename, index=False)\n",
        "print(f\"✅ Saved to {filename}\")\n",
        "\n",
        "# Download link\n",
        "from google.colab import files\n",
        "files.download(filename)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Alternative Queries\n",
        "\n",
        "Uncomment and run these cells to load different datasets."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# # Query 3: Same Product, Multiple Orders (Uniformity Check)\n",
        "# QUERY_UNIFORMITY = \"\"\"\n",
        "# WITH product_orders AS (\n",
        "#   SELECT\n",
        "#     oid.order_item_product_id,\n",
        "#     oid.order_item_title_origin AS product_title,\n",
        "#     oid.order_item_product_version_info_category AS category,\n",
        "#     ARRAY_TO_STRING(oid.order_item_product_version_thumbnail_urls, '|') AS thumbnail_urls,\n",
        "#     kse.actual_weight,\n",
        "#     kse.dimensions,\n",
        "#     kse.order_id,\n",
        "#     COUNT(*) OVER (PARTITION BY oid.order_item_product_id) AS order_count\n",
        "#   FROM `sazoshop.firestore_snapshot.v2_order_items` oid\n",
        "#   INNER JOIN `sazoshop.firestore_collection.v2_kse_cost` kse \n",
        "#     ON oid.order_item_order_id = kse.order_id\n",
        "#   WHERE\n",
        "#     kse.actual_weight > 0 AND kse.actual_weight < 50\n",
        "#     AND kse.dimensions IS NOT NULL\n",
        "#     AND REGEXP_CONTAINS(kse.dimensions, r'^\\\\d+\\\\.?\\\\d*x\\\\d+\\\\.?\\\\d*x\\\\d+\\\\.?\\\\d*$')\n",
        "#     AND oid.order_item_title_origin IS NOT NULL\n",
        "# )\n",
        "# SELECT \n",
        "#   order_item_product_id,\n",
        "#   product_title,\n",
        "#   category,\n",
        "#   ANY_VALUE(thumbnail_urls) AS thumbnail_urls,\n",
        "#   order_count,\n",
        "#   AVG(actual_weight) AS avg_actual_weight,\n",
        "#   STDDEV(actual_weight) AS stddev_actual_weight\n",
        "# FROM product_orders\n",
        "# WHERE order_count >= 3\n",
        "# GROUP BY order_item_product_id, product_title, category, order_count\n",
        "# ORDER BY order_count DESC\n",
        "# LIMIT 50\n",
        "# \"\"\"\n",
        "# \n",
        "# df_uniformity = client.query(QUERY_UNIFORMITY).to_dataframe()\n",
        "# print(f\"✅ Loaded {len(df_uniformity)} products for uniformity check\")\n",
        "# df_uniformity.head()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}