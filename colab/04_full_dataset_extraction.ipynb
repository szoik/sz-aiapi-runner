{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Full Dataset Extraction\n",
        "\n",
        "Extract all single-item orders with KSE measurements (~57K items)\n",
        "\n",
        "**Output:** CSV/Parquet file for local analysis and experiments"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "\n",
        "PROJECT_ID = \"sazoshop\"\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "print(\"Authenticated\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Check Data Size First"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Count query - check how many records we'll get\n",
        "COUNT_QUERY = \"\"\"\n",
        "WITH single_item_orders AS (\n",
        "  SELECT order_item_order_id\n",
        "  FROM `sazoshop.firestore_snapshot.v2_order_items`\n",
        "  GROUP BY order_item_order_id\n",
        "  HAVING COUNT(DISTINCT order_item_id) = 1\n",
        ")\n",
        "\n",
        "SELECT COUNT(*) as total_count\n",
        "FROM `sazoshop.firestore_snapshot.v2_order_items` oi\n",
        "INNER JOIN `sazoshop.firestore_collection.v2_kse_cost` kse\n",
        "  ON oi.order_item_order_id = kse.order_id\n",
        "WHERE \n",
        "  oi.order_item_order_id IN (SELECT order_item_order_id FROM single_item_orders)\n",
        "  AND kse.dimensions IS NOT NULL\n",
        "  AND kse.actual_weight IS NOT NULL\n",
        "  AND kse.actual_weight > 0\n",
        "  AND REGEXP_CONTAINS(kse.dimensions, r'^[0-9.]+x[0-9.]+x[0-9.]+$')\n",
        "\"\"\"\n",
        "\n",
        "count_result = client.query(COUNT_QUERY).to_dataframe()\n",
        "print(f\"Total records to extract: {count_result['total_count'].values[0]:,}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Extract Full Dataset"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Full extraction query (aligned with bigquery/dataset.sql)\n",
        "FULL_QUERY = \"\"\"\n",
        "WITH single_item_orders AS (\n",
        "  SELECT order_item_order_id\n",
        "  FROM `sazoshop.firestore_snapshot.v2_order_items`\n",
        "  GROUP BY order_item_order_id\n",
        "  HAVING COUNT(DISTINCT order_item_id) = 1\n",
        "),\n",
        "\n",
        "kse_shipping_data AS (\n",
        "  SELECT\n",
        "    kse.order_id,\n",
        "    kse.dimensions,\n",
        "    kse.actual_weight,\n",
        "    kse.volumetric_weight,\n",
        "    kse.shipping_date,\n",
        "    CAST(SPLIT(dimensions, 'x')[OFFSET(0)] AS FLOAT64) AS actual_d1,\n",
        "    CAST(SPLIT(dimensions, 'x')[OFFSET(1)] AS FLOAT64) AS actual_d2,\n",
        "    CAST(SPLIT(dimensions, 'x')[OFFSET(2)] AS FLOAT64) AS actual_d3,\n",
        "    (CAST(SPLIT(dimensions, 'x')[OFFSET(0)] AS FLOAT64) *\n",
        "     CAST(SPLIT(dimensions, 'x')[OFFSET(1)] AS FLOAT64) *\n",
        "     CAST(SPLIT(dimensions, 'x')[OFFSET(2)] AS FLOAT64)) / 1000000 AS volume_m3\n",
        "  FROM `sazoshop.firestore_collection.v2_kse_cost` kse\n",
        "  WHERE kse.order_id IN (SELECT order_item_order_id FROM single_item_orders)\n",
        "    AND kse.dimensions IS NOT NULL\n",
        "    AND kse.actual_weight IS NOT NULL\n",
        "    AND kse.actual_weight > 0\n",
        "    AND REGEXP_CONTAINS(kse.dimensions, r'^\\\\d+\\\\.?\\\\d*x\\\\d+\\\\.?\\\\d*x\\\\d+\\\\.?\\\\d*$')\n",
        "),\n",
        "\n",
        "order_item_details AS (\n",
        "  SELECT\n",
        "    oi.order_item_order_id,\n",
        "    oi.order_item_id,\n",
        "    oi.order_item_product_id,\n",
        "    oi.order_item_product_version_id,\n",
        "    oi.order_item_title_origin,\n",
        "    oi.order_item_title_target,\n",
        "    oi.order_item_product_version_info_category,\n",
        "    oi.order_item_meta_custom_category_name,\n",
        "    oi.order_item_product_version_site_name,\n",
        "    oi.order_item_product_version_details,\n",
        "    oi.order_item_product_version_info_details,\n",
        "    oi.order_item_meta_materials,\n",
        "    oi.order_item_meta_clothing_materials,\n",
        "    oi.order_item_meta_hscode,\n",
        "    ARRAY_TO_STRING(oi.order_item_product_version_thumbnail_urls, '|') AS thumbnail_urls,\n",
        "    ARRAY_LENGTH(oi.order_item_product_version_thumbnail_urls) AS thumbnail_count,\n",
        "    oi.order_item_price_origin_base,\n",
        "    oi.order_item_price_target_base,\n",
        "    oi.order_item_product_version_url,\n",
        "    SAFE_CAST(JSON_EXTRACT_SCALAR(oi.order_item_product_version_extra, '$.weight') AS FLOAT64) AS ai_weight_kg,\n",
        "    SAFE_CAST(JSON_EXTRACT_SCALAR(oi.order_item_product_version_extra, '$.volume') AS FLOAT64) AS ai_volume_m3,\n",
        "    JSON_EXTRACT_SCALAR(oi.order_item_product_version_extra, '$.width') AS ai_width,\n",
        "    JSON_EXTRACT_SCALAR(oi.order_item_product_version_extra, '$.depth') AS ai_depth,\n",
        "    JSON_EXTRACT_SCALAR(oi.order_item_product_version_extra, '$.height') AS ai_height,\n",
        "    JSON_EXTRACT_SCALAR(oi.order_item_product_version_extra, '$.volume') AS ai_volume_str,\n",
        "    JSON_EXTRACT_SCALAR(oi.order_item_product_version_extra, '$.packed_volume') AS ai_packed_volume_str,\n",
        "    oi.created_at,\n",
        "    ROW_NUMBER() OVER (PARTITION BY oi.order_item_order_id ORDER BY oi.created_at DESC) AS rn\n",
        "  FROM `sazoshop.firestore_snapshot.v2_order_items` oi\n",
        "  WHERE oi.order_item_order_id IN (SELECT order_item_order_id FROM single_item_orders)\n",
        ")\n",
        "\n",
        "SELECT\n",
        "  oid.order_item_order_id AS order_id,\n",
        "  oid.order_item_id AS item_id,\n",
        "  oid.order_item_product_id AS product_id,\n",
        "  oid.order_item_product_version_id AS product_version_id,\n",
        "  IFNULL(oid.order_item_title_origin, '') AS title_origin,\n",
        "  IFNULL(oid.order_item_title_target, '') AS title_target,\n",
        "  IFNULL(oid.order_item_product_version_info_category, '') AS category,\n",
        "  IFNULL(oid.order_item_meta_custom_category_name, '') AS custom_category,\n",
        "  IFNULL(oid.order_item_product_version_site_name, '') AS site_name,\n",
        "  IFNULL(oid.order_item_product_version_details, '') AS product_details,\n",
        "  IFNULL(oid.order_item_product_version_info_details, '') AS product_info_details,\n",
        "  IFNULL(oid.order_item_meta_materials, '') AS materials,\n",
        "  IFNULL(oid.order_item_meta_clothing_materials, '') AS clothing_materials,\n",
        "  IFNULL(oid.order_item_meta_hscode, '') AS hscode,\n",
        "  IFNULL(oid.thumbnail_urls, '') AS thumbnail_urls,\n",
        "  IFNULL(oid.thumbnail_count, 0) AS thumbnail_count,\n",
        "  IFNULL(oid.order_item_price_origin_base, 0) AS price_origin,\n",
        "  IFNULL(oid.order_item_price_target_base, 0) AS price_krw,\n",
        "  IFNULL(oid.order_item_product_version_url, '') AS product_url,\n",
        "  kse.actual_weight,\n",
        "  kse.dimensions AS actual_dimensions,\n",
        "  kse.actual_d1,\n",
        "  kse.actual_d2,\n",
        "  kse.actual_d3,\n",
        "  kse.volume_m3 AS actual_volume_m3,\n",
        "  kse.volumetric_weight,\n",
        "  oid.ai_weight_kg,\n",
        "  oid.ai_volume_m3,\n",
        "  SAFE_CAST(oid.ai_width AS FLOAT64) AS ai_width_cm,\n",
        "  SAFE_CAST(oid.ai_depth AS FLOAT64) AS ai_depth_cm,\n",
        "  SAFE_CAST(oid.ai_height AS FLOAT64) AS ai_height_cm,\n",
        "  oid.ai_volume_str,\n",
        "  oid.ai_packed_volume_str,\n",
        "  kse.shipping_date,\n",
        "  oid.created_at AS order_created_at\n",
        "FROM order_item_details oid\n",
        "INNER JOIN kse_shipping_data kse ON oid.order_item_order_id = kse.order_id\n",
        "WHERE\n",
        "  oid.rn = 1\n",
        "  AND kse.actual_weight > 0\n",
        "  AND kse.volume_m3 > 0\n",
        "  AND kse.actual_weight < 100\n",
        "  AND kse.volume_m3 < 1\n",
        "  AND (oid.order_item_title_origin IS NOT NULL OR oid.order_item_title_target IS NOT NULL)\n",
        "ORDER BY kse.shipping_date DESC\n",
        "\"\"\"\n",
        "\n",
        "print(\"Extracting full dataset... (may take 1-2 minutes)\")\n",
        "df = client.query(FULL_QUERY).to_dataframe()\n",
        "print(f\"Extracted {len(df):,} records\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Data Overview"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Column Info ===\")\n",
        "print(df.dtypes)\n",
        "print(\"\\n=== Sample Data ===\")\n",
        "df.head(3)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic stats\n",
        "print(\"=== Basic Statistics ===\")\n",
        "print(f\"Total records: {len(df):,}\")\n",
        "print(f\"Date range: {df['shipping_date'].min()} ~ {df['shipping_date'].max()}\")\n",
        "print(f\"\\nUnique products: {df['product_version_id'].nunique():,}\")\n",
        "print(f\"Unique categories: {df['category'].nunique():,}\")\n",
        "print(f\"Unique sites: {df['site_name'].nunique():,}\")\n",
        "\n",
        "print(f\"\\n=== Has AI Estimates ===\")\n",
        "has_ai = df['ai_weight_kg'].notna()\n",
        "print(f\"With AI estimates: {has_ai.sum():,} ({has_ai.mean()*100:.1f}%)\")\n",
        "print(f\"Without AI estimates: {(~has_ai).sum():,} ({(~has_ai).mean()*100:.1f}%)\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Weight distribution\n",
        "print(\"=== Actual Weight Distribution ===\")\n",
        "print(df['actual_weight'].describe())\n",
        "\n",
        "print(\"\\n=== Weight Ranges ===\")\n",
        "weight_bins = [0, 0.5, 1, 2, 5, 10, 100]\n",
        "labels = ['0-0.5kg', '0.5-1kg', '1-2kg', '2-5kg', '5-10kg', '10kg+']\n",
        "df['weight_range'] = pd.cut(df['actual_weight'], bins=weight_bins, labels=labels)\n",
        "print(df['weight_range'].value_counts().sort_index())"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top categories\n",
        "print(\"=== Top 20 Categories ===\")\n",
        "print(df['category'].value_counts().head(20))"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Add Computed Columns"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort actual dimensions (max >= mid >= min)\n",
        "def sort_dims(d1, d2, d3):\n",
        "    dims = sorted([d1, d2, d3], reverse=True)\n",
        "    return dims[0], dims[1], dims[2]\n",
        "\n",
        "# Original dimensions are kept as actual_d1, actual_d2, actual_d3 from BigQuery\n",
        "# Add sorted dimensions for comparison\n",
        "sorted_dims = df.apply(lambda r: sort_dims(r['actual_d1'], r['actual_d2'], r['actual_d3']), axis=1)\n",
        "df['actual_max'] = sorted_dims.apply(lambda x: x[0])\n",
        "df['actual_mid'] = sorted_dims.apply(lambda x: x[1])\n",
        "df['actual_min'] = sorted_dims.apply(lambda x: x[2])\n",
        "\n",
        "# Calculate actual volume (cm³ and liters) - volume_m3 already from BigQuery\n",
        "df['actual_volume_cm3'] = df['actual_volume_m3'] * 1000000  # m³ -> cm³\n",
        "df['actual_volume_L'] = df['actual_volume_m3'] * 1000  # m³ -> L\n",
        "\n",
        "print(\"Added columns:\")\n",
        "print(\"  - Original: actual_d1, actual_d2, actual_d3 (from KSE)\")\n",
        "print(\"  - Sorted: actual_max, actual_mid, actual_min (for comparison)\")\n",
        "print(\"  - Volume: actual_volume_cm3, actual_volume_L (converted from actual_volume_m3)\")\n",
        "df[['actual_dimensions', 'actual_d1', 'actual_d2', 'actual_d3', 'actual_max', 'actual_mid', 'actual_min']].head()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort AI dimensions for comparison (ai_width_cm, ai_depth_cm, ai_height_cm already from BigQuery)\n",
        "def sort_ai_dims(row):\n",
        "    w, d, h = row['ai_width_cm'], row['ai_depth_cm'], row['ai_height_cm']\n",
        "    if pd.notna(w) and pd.notna(d) and pd.notna(h):\n",
        "        dims = sorted([w, d, h], reverse=True)\n",
        "        return pd.Series({'ai_max': dims[0], 'ai_mid': dims[1], 'ai_min': dims[2]})\n",
        "    return pd.Series({'ai_max': None, 'ai_mid': None, 'ai_min': None})\n",
        "\n",
        "ai_sorted = df.apply(sort_ai_dims, axis=1)\n",
        "df = pd.concat([df, ai_sorted], axis=1)\n",
        "\n",
        "# Calculate AI volume in cm³ and L (ai_volume_m3 already from BigQuery)\n",
        "df['ai_volume_cm3'] = df['ai_volume_m3'] * 1000000  # m³ -> cm³\n",
        "df['ai_volume_L'] = df['ai_volume_m3'] * 1000  # m³ -> L\n",
        "\n",
        "print(\"Added columns:\")\n",
        "print(\"  - Original: ai_width_cm, ai_depth_cm, ai_height_cm (from BigQuery)\")\n",
        "print(\"  - Sorted: ai_max, ai_mid, ai_min (for comparison)\")\n",
        "print(\"  - Volume: ai_volume_cm3, ai_volume_L (converted from ai_volume_m3)\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate errors (for items with AI estimates)\n",
        "# Signed error: positive = AI overestimated, negative = AI underestimated\n",
        "# Using SORTED dimensions for fair comparison\n",
        "mask = df['ai_weight_kg'].notna()\n",
        "\n",
        "# Weight and volume errors\n",
        "df.loc[mask, 'weight_error'] = (df.loc[mask, 'ai_weight_kg'] - df.loc[mask, 'actual_weight']) / df.loc[mask, 'actual_weight']\n",
        "df.loc[mask, 'volume_error'] = (df.loc[mask, 'ai_volume_cm3'] - df.loc[mask, 'actual_volume_cm3']) / df.loc[mask, 'actual_volume_cm3']\n",
        "\n",
        "# Dimension errors (using sorted: max/mid/min)\n",
        "df.loc[mask, 'max_error'] = (df.loc[mask, 'ai_max'] - df.loc[mask, 'actual_max']) / df.loc[mask, 'actual_max']\n",
        "df.loc[mask, 'mid_error'] = (df.loc[mask, 'ai_mid'] - df.loc[mask, 'actual_mid']) / df.loc[mask, 'actual_mid']\n",
        "df.loc[mask, 'min_error'] = (df.loc[mask, 'ai_min'] - df.loc[mask, 'actual_min']) / df.loc[mask, 'actual_min']\n",
        "df.loc[mask, 'avg_dim_error'] = (df.loc[mask, 'max_error'] + df.loc[mask, 'mid_error'] + df.loc[mask, 'min_error']) / 3\n",
        "\n",
        "print(\"Added error columns (signed: + = overestimate, - = underestimate)\")\n",
        "print(\"  - weight_error, volume_error\")\n",
        "print(\"  - max_error, mid_error, min_error (sorted dimensions)\")\n",
        "print(\"  - avg_dim_error\")\n",
        "print(f\"\\nItems with errors calculated: {mask.sum():,}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Save Dataset"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Select columns to save\n",
        "columns_to_save = [\n",
        "    # IDs\n",
        "    'order_id', 'item_id', 'product_id', 'product_version_id',\n",
        "    # Input features - text\n",
        "    'title_origin', 'title_target', 'category', 'custom_category',\n",
        "    'site_name', 'product_details', 'product_info_details',\n",
        "    'materials', 'clothing_materials', 'hscode',\n",
        "    # Input features - image/price/url\n",
        "    'thumbnail_urls', 'thumbnail_count', 'price_origin', 'price_krw', 'product_url',\n",
        "    # Actual - original order (from KSE)\n",
        "    'actual_weight', 'actual_dimensions', 'actual_d1', 'actual_d2', 'actual_d3',\n",
        "    # Actual - sorted (for comparison)\n",
        "    'actual_max', 'actual_mid', 'actual_min',\n",
        "    'actual_volume_m3', 'actual_volume_cm3', 'actual_volume_L', 'volumetric_weight',\n",
        "    # AI estimates - original order\n",
        "    'ai_weight_kg', 'ai_width_cm', 'ai_depth_cm', 'ai_height_cm',\n",
        "    # AI estimates - sorted (for comparison)\n",
        "    'ai_max', 'ai_mid', 'ai_min',\n",
        "    'ai_volume_m3', 'ai_volume_cm3', 'ai_volume_L',\n",
        "    'ai_volume_str', 'ai_packed_volume_str',\n",
        "    # Errors (using sorted dimensions)\n",
        "    'weight_error', 'volume_error', 'max_error', 'mid_error', 'min_error', 'avg_dim_error',\n",
        "    # Metadata\n",
        "    'shipping_date', 'order_created_at', 'weight_range'\n",
        "]\n",
        "\n",
        "df_export = df[columns_to_save].copy()\n",
        "print(f\"Columns to export: {len(columns_to_save)}\")\n",
        "print(f\"Rows: {len(df_export):,}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save as CSV\n",
        "from datetime import datetime\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d')\n",
        "csv_filename = f'single_item_kse_full_{timestamp}.csv'\n",
        "\n",
        "df_export.to_csv(csv_filename, index=False)\n",
        "print(f\"Saved: {csv_filename}\")\n",
        "\n",
        "import os\n",
        "file_size_mb = os.path.getsize(csv_filename) / 1024 / 1024\n",
        "print(f\"File size: {file_size_mb:.1f} MB\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Also save as Parquet (smaller, faster to load)\n",
        "parquet_filename = f'single_item_kse_full_{timestamp}.parquet'\n",
        "df_export.to_parquet(parquet_filename, index=False)\n",
        "\n",
        "parquet_size_mb = os.path.getsize(parquet_filename) / 1024 / 1024\n",
        "print(f\"Saved: {parquet_filename}\")\n",
        "print(f\"File size: {parquet_size_mb:.1f} MB (Parquet is more compact)\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download files\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Downloading CSV...\")\n",
        "files.download(csv_filename)\n",
        "\n",
        "# Uncomment to also download parquet\n",
        "# print(\"Downloading Parquet...\")\n",
        "# files.download(parquet_filename)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Quick Filtering Examples\n",
        "\n",
        "Once you have this dataset, you can filter for various experiments:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Example filters you can apply later\n",
        "\n",
        "# 1. Items WITHOUT AI estimates (need to run estimation)\n",
        "no_ai = df_export[df_export['ai_weight_kg'].isna()]\n",
        "print(f\"Items needing AI estimation: {len(no_ai):,}\")\n",
        "\n",
        "# 2. Items WITH AI estimates (can calculate errors)\n",
        "has_ai = df_export[df_export['ai_weight_kg'].notna()]\n",
        "print(f\"Items with AI estimates: {len(has_ai):,}\")\n",
        "\n",
        "# 3. Overestimated items (AI estimate > actual)\n",
        "overestimated = df_export[df_export['weight_error'] > 0.5]  # AI estimated 50%+ higher\n",
        "print(f\"Weight overestimated by >50%: {len(overestimated):,}\")\n",
        "\n",
        "# 4. Underestimated items (AI estimate < actual)\n",
        "underestimated = df_export[df_export['weight_error'] < -0.5]  # AI estimated 50%+ lower\n",
        "print(f\"Weight underestimated by >50%: {len(underestimated):,}\")\n",
        "\n",
        "# 5. Specific category\n",
        "perfume = df_export[df_export['category'].str.contains('향수|퍼퓸|perfume', case=False, na=False)]\n",
        "print(f\"Perfume items: {len(perfume):,}\")\n",
        "\n",
        "# 6. Weight range\n",
        "light = df_export[df_export['actual_weight'] < 0.5]\n",
        "heavy = df_export[df_export['actual_weight'] > 5]\n",
        "print(f\"Light items (<0.5kg): {len(light):,}\")\n",
        "print(f\"Heavy items (>5kg): {len(heavy):,}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Done!\n",
        "\n",
        "You now have the full dataset. Next steps:\n",
        "1. Download the CSV/Parquet file\n",
        "2. Use it locally or upload to Google Drive\n",
        "3. Filter by various conditions for experiments\n",
        "4. Run new AI estimations on items without estimates"
      ],
      "metadata": {}
    }
  ]
}
