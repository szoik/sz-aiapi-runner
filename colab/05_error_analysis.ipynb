{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis - Weight & Volume Predictions\n",
    "\n",
    "Analyze prediction errors from the experiment datasource:\n",
    "- Top 200 errors (volume, weight, both)\n",
    "- Dimension order issues (L x W x H twisted)\n",
    "- Error distribution by category\n",
    "- Positive vs negative errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the TSV file\n",
    "from google.colab import files\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Upload 20260128_experiment_datasource.tsv\")\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "filename = list(uploaded.keys())[0]\n",
    "df = pd.read_csv(filename, sep='\\t')\n",
    "print(f\"Loaded {len(df)} rows\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick overview of error columns\n",
    "error_cols = ['weight_error', 'volume_error', 'max_error', 'mid_error', 'min_error', 'avg_dim_error']\n",
    "df[error_cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Top 200 Volume Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 200 Volume Errors - POSITIVE (AI overestimated)\n",
    "top_volume_positive = df.nlargest(200, 'volume_error')[[\n",
    "    'product_version_id', 'title_origin', 'category', \n",
    "    'volume_error', 'weight_error',\n",
    "    'actual_volume_cm3', 'ai_volume_cm3',\n",
    "    'actual_max', 'actual_mid', 'actual_min',\n",
    "    'ai_max', 'ai_mid', 'ai_min',\n",
    "    'thumbnail_urls'\n",
    "]]\n",
    "print(\"Top 200 POSITIVE Volume Errors (AI overestimated)\")\n",
    "print(f\"Error range: {top_volume_positive['volume_error'].min():.2%} ~ {top_volume_positive['volume_error'].max():.2%}\")\n",
    "top_volume_positive.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 200 Volume Errors - NEGATIVE (AI underestimated)\n",
    "top_volume_negative = df.nsmallest(200, 'volume_error')[[\n",
    "    'product_version_id', 'title_origin', 'category', \n",
    "    'volume_error', 'weight_error',\n",
    "    'actual_volume_cm3', 'ai_volume_cm3',\n",
    "    'actual_max', 'actual_mid', 'actual_min',\n",
    "    'ai_max', 'ai_mid', 'ai_min',\n",
    "    'thumbnail_urls'\n",
    "]]\n",
    "print(\"Top 200 NEGATIVE Volume Errors (AI underestimated)\")\n",
    "print(f\"Error range: {top_volume_negative['volume_error'].min():.2%} ~ {top_volume_negative['volume_error'].max():.2%}\")\n",
    "top_volume_negative.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Top 200 Weight Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 200 Weight Errors - POSITIVE (AI overestimated)\n",
    "top_weight_positive = df.nlargest(200, 'weight_error')[[\n",
    "    'product_version_id', 'title_origin', 'category', \n",
    "    'weight_error', 'volume_error',\n",
    "    'actual_weight', 'ai_weight_kg',\n",
    "    'thumbnail_urls'\n",
    "]]\n",
    "print(\"Top 200 POSITIVE Weight Errors (AI overestimated)\")\n",
    "print(f\"Error range: {top_weight_positive['weight_error'].min():.2%} ~ {top_weight_positive['weight_error'].max():.2%}\")\n",
    "top_weight_positive.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 200 Weight Errors - NEGATIVE (AI underestimated)\n",
    "top_weight_negative = df.nsmallest(200, 'weight_error')[[\n",
    "    'product_version_id', 'title_origin', 'category', \n",
    "    'weight_error', 'volume_error',\n",
    "    'actual_weight', 'ai_weight_kg',\n",
    "    'thumbnail_urls'\n",
    "]]\n",
    "print(\"Top 200 NEGATIVE Weight Errors (AI underestimated)\")\n",
    "print(f\"Error range: {top_weight_negative['weight_error'].min():.2%} ~ {top_weight_negative['weight_error'].max():.2%}\")\n",
    "top_weight_negative.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Top 200 Combined Errors (Both Volume AND Weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined error: average of absolute errors\n",
    "df['combined_error'] = (df['weight_error'].abs() + df['volume_error'].abs()) / 2\n",
    "\n",
    "# Top 200 Combined Errors\n",
    "top_combined = df.nlargest(200, 'combined_error')[[\n",
    "    'product_version_id', 'title_origin', 'category', \n",
    "    'weight_error', 'volume_error', 'combined_error',\n",
    "    'actual_weight', 'ai_weight_kg',\n",
    "    'actual_volume_cm3', 'ai_volume_cm3',\n",
    "    'thumbnail_urls'\n",
    "]]\n",
    "print(\"Top 200 Combined Errors (high error in BOTH weight and volume)\")\n",
    "print(f\"Combined error range: {top_combined['combined_error'].min():.2%} ~ {top_combined['combined_error'].max():.2%}\")\n",
    "top_combined.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dimension Order Twisted (L x W x H)\n",
    "\n",
    "Check if AI dimensions are in different order than actual dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dimension order issues\n",
    "# Compare which actual dimension matches which AI dimension\n",
    "\n",
    "def check_dimension_twist(row):\n",
    "    \"\"\"Check if dimensions are twisted between actual and AI.\"\"\"\n",
    "    actual = [row['actual_max'], row['actual_mid'], row['actual_min']]\n",
    "    ai = [row['ai_max'], row['ai_mid'], row['ai_min']]\n",
    "    \n",
    "    # Check if any are NaN\n",
    "    if any(pd.isna(actual)) or any(pd.isna(ai)):\n",
    "        return 'unknown'\n",
    "    \n",
    "    # Both should be sorted max >= mid >= min\n",
    "    # Check if the relative ordering is preserved\n",
    "    actual_sorted = sorted(actual, reverse=True)\n",
    "    ai_sorted = sorted(ai, reverse=True)\n",
    "    \n",
    "    # Calculate which AI dimension is closest to which actual dimension\n",
    "    # This detects if AI swapped dimensions\n",
    "    max_ratio = ai[0] / actual[0] if actual[0] > 0 else 0\n",
    "    mid_ratio = ai[1] / actual[1] if actual[1] > 0 else 0\n",
    "    min_ratio = ai[2] / actual[2] if actual[2] > 0 else 0\n",
    "    \n",
    "    # If ratios are very different, dimensions might be twisted\n",
    "    ratios = [max_ratio, mid_ratio, min_ratio]\n",
    "    ratio_std = np.std(ratios)\n",
    "    \n",
    "    if ratio_std > 0.5:  # High variance in ratios suggests twist\n",
    "        return 'twisted'\n",
    "    return 'normal'\n",
    "\n",
    "df['dim_twist'] = df.apply(check_dimension_twist, axis=1)\n",
    "print(\"Dimension Twist Analysis:\")\n",
    "print(df['dim_twist'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show twisted dimension items\n",
    "twisted_items = df[df['dim_twist'] == 'twisted'][[\n",
    "    'product_version_id', 'title_origin', 'category',\n",
    "    'actual_max', 'actual_mid', 'actual_min',\n",
    "    'ai_max', 'ai_mid', 'ai_min',\n",
    "    'max_error', 'mid_error', 'min_error',\n",
    "    'volume_error',\n",
    "    'thumbnail_urls'\n",
    "]]\n",
    "print(f\"Twisted dimension items: {len(twisted_items)}\")\n",
    "twisted_items.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error Distribution by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize by error level\n",
    "def categorize_error(error):\n",
    "    abs_err = abs(error)\n",
    "    if abs_err > 1.0:  # > 100%\n",
    "        return 'over_100%'\n",
    "    elif abs_err > 0.5:  # 50% ~ 100%\n",
    "        return '50%_to_100%'\n",
    "    elif abs_err > 0.1:  # 10% ~ 50%\n",
    "        return '10%_to_50%'\n",
    "    else:  # < 10%\n",
    "        return 'under_10%'\n",
    "\n",
    "df['weight_error_level'] = df['weight_error'].apply(categorize_error)\n",
    "df['volume_error_level'] = df['volume_error'].apply(categorize_error)\n",
    "\n",
    "print(\"Weight Error Distribution:\")\n",
    "print(df['weight_error_level'].value_counts())\n",
    "print(\"\\nVolume Error Distribution:\")\n",
    "print(df['volume_error_level'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Over 100% Error Items by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over 100% VOLUME error - POSITIVE\n",
    "over_100_vol_pos = df[(df['volume_error'] > 1.0)]\n",
    "print(f\"Volume Error > +100%: {len(over_100_vol_pos)} items\")\n",
    "print(\"\\nBy Category:\")\n",
    "print(over_100_vol_pos['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over 100% VOLUME error - NEGATIVE\n",
    "over_100_vol_neg = df[(df['volume_error'] < -0.5)]  # -50% means actual is 2x AI, -100% would be impossible\n",
    "print(f\"Volume Error < -50%: {len(over_100_vol_neg)} items\")\n",
    "print(\"\\nBy Category:\")\n",
    "print(over_100_vol_neg['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over 100% WEIGHT error - POSITIVE\n",
    "over_100_wt_pos = df[(df['weight_error'] > 1.0)]\n",
    "print(f\"Weight Error > +100%: {len(over_100_wt_pos)} items\")\n",
    "print(\"\\nBy Category:\")\n",
    "print(over_100_wt_pos['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over 100% WEIGHT error - NEGATIVE\n",
    "over_100_wt_neg = df[(df['weight_error'] < -0.5)]\n",
    "print(f\"Weight Error < -50%: {len(over_100_wt_neg)} items\")\n",
    "print(\"\\nBy Category:\")\n",
    "print(over_100_wt_neg['category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 50% ~ 100% Error Items by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50% ~ 100% VOLUME error - POSITIVE\n",
    "mid_vol_pos = df[(df['volume_error'] > 0.5) & (df['volume_error'] <= 1.0)]\n",
    "print(f\"Volume Error +50% ~ +100%: {len(mid_vol_pos)} items\")\n",
    "print(\"\\nBy Category:\")\n",
    "print(mid_vol_pos['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50% ~ 100% VOLUME error - NEGATIVE\n",
    "mid_vol_neg = df[(df['volume_error'] < -0.33) & (df['volume_error'] >= -0.5)]  # -33% to -50%\n",
    "print(f\"Volume Error -33% ~ -50%: {len(mid_vol_neg)} items\")\n",
    "print(\"\\nBy Category:\")\n",
    "print(mid_vol_neg['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50% ~ 100% WEIGHT error - POSITIVE\n",
    "mid_wt_pos = df[(df['weight_error'] > 0.5) & (df['weight_error'] <= 1.0)]\n",
    "print(f\"Weight Error +50% ~ +100%: {len(mid_wt_pos)} items\")\n",
    "print(\"\\nBy Category:\")\n",
    "print(mid_wt_pos['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50% ~ 100% WEIGHT error - NEGATIVE\n",
    "mid_wt_neg = df[(df['weight_error'] < -0.33) & (df['weight_error'] >= -0.5)]\n",
    "print(f\"Weight Error -33% ~ -50%: {len(mid_wt_neg)} items\")\n",
    "print(\"\\nBy Category:\")\n",
    "print(mid_wt_neg['category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Under 10% Error Items by Category (Good predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under 10% VOLUME error - POSITIVE\n",
    "good_vol_pos = df[(df['volume_error'] > 0) & (df['volume_error'] <= 0.1)]\n",
    "print(f\"Volume Error 0% ~ +10%: {len(good_vol_pos)} items\")\n",
    "print(\"\\nBy Category:\")\n",
    "print(good_vol_pos['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under 10% VOLUME error - NEGATIVE\n",
    "good_vol_neg = df[(df['volume_error'] < 0) & (df['volume_error'] >= -0.1)]\n",
    "print(f\"Volume Error -10% ~ 0%: {len(good_vol_neg)} items\")\n",
    "print(\"\\nBy Category:\")\n",
    "print(good_vol_neg['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under 10% WEIGHT error - POSITIVE\n",
    "good_wt_pos = df[(df['weight_error'] > 0) & (df['weight_error'] <= 0.1)]\n",
    "print(f\"Weight Error 0% ~ +10%: {len(good_wt_pos)} items\")\n",
    "print(\"\\nBy Category:\")\n",
    "print(good_wt_pos['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under 10% WEIGHT error - NEGATIVE\n",
    "good_wt_neg = df[(df['weight_error'] < 0) & (df['weight_error'] >= -0.1)]\n",
    "print(f\"Weight Error -10% ~ 0%: {len(good_wt_neg)} items\")\n",
    "print(\"\\nBy Category:\")\n",
    "print(good_wt_neg['category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary Statistics by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary by category\n",
    "category_stats = df.groupby('category').agg({\n",
    "    'weight_error': ['mean', 'std', 'min', 'max', 'count'],\n",
    "    'volume_error': ['mean', 'std', 'min', 'max'],\n",
    "}).round(4)\n",
    "\n",
    "print(\"Error Statistics by Category:\")\n",
    "category_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories with highest average absolute error\n",
    "df['abs_weight_error'] = df['weight_error'].abs()\n",
    "df['abs_volume_error'] = df['volume_error'].abs()\n",
    "\n",
    "worst_categories = df.groupby('category').agg({\n",
    "    'abs_weight_error': 'mean',\n",
    "    'abs_volume_error': 'mean',\n",
    "    'product_version_id': 'count'\n",
    "}).rename(columns={'product_version_id': 'count'}).sort_values('abs_volume_error', ascending=False)\n",
    "\n",
    "print(\"Categories by Average Absolute Error (worst first):\")\n",
    "worst_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Weight error histogram\n",
    "ax1 = axes[0, 0]\n",
    "df['weight_error'].clip(-2, 2).hist(bins=50, ax=ax1, color='steelblue', edgecolor='white')\n",
    "ax1.axvline(x=0, color='red', linestyle='--', linewidth=1)\n",
    "ax1.set_title('Weight Error Distribution (clipped to ±200%)')\n",
    "ax1.set_xlabel('Error (signed)')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "# Volume error histogram\n",
    "ax2 = axes[0, 1]\n",
    "df['volume_error'].clip(-2, 2).hist(bins=50, ax=ax2, color='darkorange', edgecolor='white')\n",
    "ax2.axvline(x=0, color='red', linestyle='--', linewidth=1)\n",
    "ax2.set_title('Volume Error Distribution (clipped to ±200%)')\n",
    "ax2.set_xlabel('Error (signed)')\n",
    "ax2.set_ylabel('Count')\n",
    "\n",
    "# Scatter: weight vs volume error\n",
    "ax3 = axes[1, 0]\n",
    "ax3.scatter(df['weight_error'].clip(-2, 2), df['volume_error'].clip(-2, 2), alpha=0.3, s=10)\n",
    "ax3.axhline(y=0, color='red', linestyle='--', linewidth=0.5)\n",
    "ax3.axvline(x=0, color='red', linestyle='--', linewidth=0.5)\n",
    "ax3.set_title('Weight Error vs Volume Error')\n",
    "ax3.set_xlabel('Weight Error')\n",
    "ax3.set_ylabel('Volume Error')\n",
    "\n",
    "# Error level pie chart\n",
    "ax4 = axes[1, 1]\n",
    "error_counts = df['volume_error_level'].value_counts()\n",
    "ax4.pie(error_counts.values, labels=error_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "ax4.set_title('Volume Error Level Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error by category bar chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Top 10 categories by count\n",
    "top_cats = df['category'].value_counts().head(10).index\n",
    "df_top = df[df['category'].isin(top_cats)]\n",
    "\n",
    "# Weight error by category\n",
    "ax1 = axes[0]\n",
    "df_top.boxplot(column='weight_error', by='category', ax=ax1)\n",
    "ax1.set_title('Weight Error by Category (Top 10)')\n",
    "ax1.set_xlabel('Category')\n",
    "ax1.set_ylabel('Weight Error')\n",
    "ax1.set_ylim(-2, 2)\n",
    "plt.suptitle('')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Volume error by category\n",
    "ax2 = axes[1]\n",
    "df_top.boxplot(column='volume_error', by='category', ax=ax2)\n",
    "ax2.set_title('Volume Error by Category (Top 10)')\n",
    "ax2.set_xlabel('Category')\n",
    "ax2.set_ylabel('Volume Error')\n",
    "ax2.set_ylim(-2, 2)\n",
    "plt.suptitle('')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export top errors for review\n",
    "top_volume_positive.to_csv('top200_volume_error_positive.csv', index=False)\n",
    "top_volume_negative.to_csv('top200_volume_error_negative.csv', index=False)\n",
    "top_weight_positive.to_csv('top200_weight_error_positive.csv', index=False)\n",
    "top_weight_negative.to_csv('top200_weight_error_negative.csv', index=False)\n",
    "top_combined.to_csv('top200_combined_error.csv', index=False)\n",
    "twisted_items.to_csv('twisted_dimensions.csv', index=False)\n",
    "\n",
    "print(\"Exported CSV files:\")\n",
    "print(\"- top200_volume_error_positive.csv\")\n",
    "print(\"- top200_volume_error_negative.csv\")\n",
    "print(\"- top200_weight_error_positive.csv\")\n",
    "print(\"- top200_weight_error_negative.csv\")\n",
    "print(\"- top200_combined_error.csv\")\n",
    "print(\"- twisted_dimensions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download exported files\n",
    "from google.colab import files\n",
    "\n",
    "files.download('top200_volume_error_positive.csv')\n",
    "files.download('top200_volume_error_negative.csv')\n",
    "files.download('top200_weight_error_positive.csv')\n",
    "files.download('top200_weight_error_negative.csv')\n",
    "files.download('top200_combined_error.csv')\n",
    "files.download('twisted_dimensions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
